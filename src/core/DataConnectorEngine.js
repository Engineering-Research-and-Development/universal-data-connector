const EventEmitter = require('events');
const logger = require('../utils/logger');
const configManager = require('../config/ConfigManager');
const ConnectorFactory = require('../connectors/ConnectorFactory');
const DataProcessor = require('./DataProcessor');
const DataStore = require('./DataStore');
const NatsTransport = require('../transport/NatsTransport');
const { MappingEngine } = require('../mappingTools');

//const MappingEngine = require('../mappingTools/MappingEngine');

class DataConnectorEngine extends EventEmitter {
  constructor(storageConfig = null, mappingConfig = null) {
    super();
    this.connectors = new Map();
    this.dataProcessor = new DataProcessor();
    this.dataStore = new DataStore(storageConfig);
    this.mappingEngine = new MappingEngine(mappingConfig || {
      namespace: 'urn:ngsi-ld:industry50'
    });
    this.natsTransport = new NatsTransport();
    this.isRunning = false;
    this.stats = {
      totalDataPoints: 0,
      totalErrors: 0,
      startTime: null,
      lastDataReceived: null
    };
  }

  async initialize() {
    try {
      logger.info('Initializing Data Connector Engine...');

      // Initialize data processor
      await this.dataProcessor.initialize();

<<<<<<< HEAD
      // Initialize data store
      await this.dataStore.initialize();

=======
      // Initialize data store (acts as buffer now if NATS is primary)
      await this.dataStore.initialize();

      // Initialize NATS Transport
      await this.natsTransport.initialize();

      this.natsTransport.on('connected', () => {
        logger.info('NATS Transport connected. Ready to publish/flush.');
        this.flushBuffer();
      });

      this.natsTransport.on('error', (err) => {
        logger.warn('NATS Transport error:', err);
      });

>>>>>>> 5293766d3637adb1bd0c0c10ed25a5d55e1e9389
      // Setup data processor event handlers
      this.dataProcessor.on('processed', (data) => {
        this.handleProcessedData(data);
      });

      this.dataProcessor.on('error', (error) => {
        logger.error('Data processor error:', error);
        this.stats.totalErrors++;
        this.emit('processingError', error);
      });

      // Load and initialize connectors
      await this.initializeConnectors();

      logger.info('Data Connector Engine initialized successfully');
    } catch (error) {
      logger.error('Failed to initialize Data Connector Engine:', error);
      throw error;
    }
  }

  async initializeConnectors() {
    const sources = configManager.getEnabledSources();

    logger.info(`Initializing ${sources.length} connectors...`);

    for (const sourceConfig of sources) {
      try {
        await this.createConnector(sourceConfig);
      } catch (error) {
        logger.error(`Failed to initialize connector '${sourceConfig.id}':`, error);
        // Continue with other connectors even if one fails
      }
    }
  }

  async createConnector(sourceConfig) {
    try {
      // Create connector instance
      const connector = ConnectorFactory.create(sourceConfig.type, sourceConfig);

      // Setup event handlers
      this.setupConnectorEventHandlers(connector, sourceConfig.id);

      // Initialize connector
      await connector.initialize();

      // Store connector
      this.connectors.set(sourceConfig.id, {
        connector,
        config: sourceConfig,
        status: 'initialized',
        lastActivity: new Date(),
        stats: {
          dataPoints: 0,
          errors: 0,
          connections: 0
        }
      });

      logger.info(`Connector '${sourceConfig.id}' (${sourceConfig.type}) initialized successfully`);
      this.emit('connectorInitialized', sourceConfig.id);

    } catch (error) {
      logger.error(`Failed to create connector '${sourceConfig.id}':`, error);
      throw error;
    }
  }

  // Dynamic configuration reload methods
  async reloadSourcesConfiguration(newSources = null) {
    try {
      logger.info('Reloading sources configuration...');

      // Save new sources configuration if provided
      if (newSources) {
        const config = { sources: newSources };
        configManager.saveConfig(config);
        logger.info('New sources configuration saved');
      } else {
        // Reload from file
        configManager.loadConfig();
        logger.info('Sources configuration reloaded from file');
      }

      // Get current enabled sources
      const enabledSources = configManager.getEnabledSources();
      const currentConnectorIds = Array.from(this.connectors.keys());
      const newConnectorIds = enabledSources.map(s => s.id);

      // Stop and remove connectors that are no longer configured
      for (const connectorId of currentConnectorIds) {
        if (!newConnectorIds.includes(connectorId)) {
          await this.removeConnector(connectorId);
        }
      }

      // Update or create connectors
      for (const sourceConfig of enabledSources) {
        const existingConnector = this.connectors.get(sourceConfig.id);

        if (existingConnector) {
          // Check if configuration changed
          const configChanged = JSON.stringify(existingConnector.config) !== JSON.stringify(sourceConfig);

          if (configChanged) {
            logger.info(`Updating connector '${sourceConfig.id}' due to configuration change`);
            await this.updateConnector(sourceConfig.id, sourceConfig);
          }
        } else {
          // Create new connector
          logger.info(`Creating new connector '${sourceConfig.id}'`);
          await this.createConnector(sourceConfig);

          // Auto-start if engine is running
          if (this.isRunning) {
            await this.startConnector(sourceConfig.id);
          }
        }
      }

      this.emit('sourcesConfigurationReloaded', {
        totalSources: enabledSources.length,
        activeConnectors: this.connectors.size
      });

      logger.info('Sources configuration reloaded successfully');
      return {
        success: true,
        totalSources: enabledSources.length,
        activeConnectors: this.connectors.size,
        message: 'Sources configuration reloaded successfully'
      };

    } catch (error) {
      logger.error('Failed to reload sources configuration:', error);
      this.emit('sourcesConfigurationError', error);
      throw error;
    }
  }

  async updateConnector(connectorId, newConfig) {
    try {
      const existingConnector = this.connectors.get(connectorId);
      if (!existingConnector) {
        throw new Error(`Connector '${connectorId}' not found`);
      }

      const wasRunning = existingConnector.status === 'connected';

      // Stop existing connector
      if (wasRunning) {
        await this.stopConnector(connectorId);
      }

      // Remove old connector
      await this.removeConnector(connectorId);

      // Create new connector with updated config
      await this.createConnector(newConfig);

      // Start if it was running before
      if (wasRunning) {
        await this.startConnector(connectorId);
      }

      logger.info(`Connector '${connectorId}' updated successfully`);

    } catch (error) {
      logger.error(`Failed to update connector '${connectorId}':`, error);
      throw error;
    }
  }

  async removeConnector(connectorId) {
    try {
      const connectorInfo = this.connectors.get(connectorId);
      if (!connectorInfo) {
        logger.warn(`Connector '${connectorId}' not found for removal`);
        return;
      }

      // Stop connector if running
      if (connectorInfo.status === 'connected') {
        await this.stopConnector(connectorId);
      }

      // Cleanup connector
      if (connectorInfo.connector && typeof connectorInfo.connector.cleanup === 'function') {
        await connectorInfo.connector.cleanup();
      }

      // Remove from map
      this.connectors.delete(connectorId);

      logger.info(`Connector '${connectorId}' removed successfully`);
      this.emit('connectorRemoved', connectorId);

    } catch (error) {
      logger.error(`Failed to remove connector '${connectorId}':`, error);
      throw error;
    }
  }

  async reloadStorageConfiguration(newStorageConfig = null) {
    try {
      logger.info('Reloading storage configuration...');

      // Save new storage configuration if provided
      if (newStorageConfig) {
        const StorageConfigManager = require('../config/StorageConfigManager');
        await StorageConfigManager.updateStorageConfig(newStorageConfig);
        logger.info('New storage configuration saved');
      }

      // Backup current data if needed
<<<<<<< HEAD
      const currentData = await this.dataStore.getAll();
      logger.info(`Backing up ${currentData.length} data points`);
=======
      const currentData = await this.dataStore.getAll(); // Note: getAll() doesn't exist on DataStore but internal implementation uses .data
      // Actually checking DataStore.js, it has getLatest etc. We should use getLatest with max.
      // But for buffering logic, we need to respect it.
>>>>>>> 5293766d3637adb1bd0c0c10ed25a5d55e1e9389

      // Reinitialize data store with new configuration
      this.dataStore = new DataStore(newStorageConfig);
      await this.dataStore.initialize();

<<<<<<< HEAD
      // Restore data if any
      if (currentData.length > 0) {
        logger.info(`Restoring ${currentData.length} data points to new storage`);
        for (const dataPoint of currentData) {
          await this.dataStore.store(dataPoint);
        }
      }
=======
      // Restore data if any (assuming memory migration or buffer preservation)
      // For now we skip complex migration of buffer during config reload unless critical
>>>>>>> 5293766d3637adb1bd0c0c10ed25a5d55e1e9389

      this.emit('storageConfigurationReloaded', {
        storageType: newStorageConfig?.type || 'memory'
      });

      logger.info('Storage configuration reloaded successfully');
      return {
        success: true,
        storageType: newStorageConfig?.type || 'memory',
        message: 'Storage configuration reloaded successfully'
      };

    } catch (error) {
      logger.error('Failed to reload storage configuration:', error);
      this.emit('storageConfigurationError', error);
      throw error;
    }
  }

  setupConnectorEventHandlers(connector, sourceId) {
    connector.on('data', (data) => {
      this.handleConnectorData(sourceId, data);
    });

    connector.on('connected', () => {
      this.updateConnectorStatus(sourceId, 'connected');
      logger.info(`Connector '${sourceId}' connected`);
    });

    connector.on('disconnected', () => {
      this.updateConnectorStatus(sourceId, 'disconnected');
      logger.warn(`Connector '${sourceId}' disconnected`);
    });

    connector.on('error', (error) => {
      this.handleConnectorError(sourceId, error);
    });

    connector.on('reconnecting', () => {
      this.updateConnectorStatus(sourceId, 'reconnecting');
      logger.info(`Connector '${sourceId}' reconnecting...`);
    });

    // Discovery Events for Auto-Mapping
    connector.on('nodesDiscovered', (discoveryData) => {
      logger.info(`Connector '${sourceId}' discovered nodes. Processing for mapping...`);
      this.emit('nodesDiscovered', discoveryData);
      // Here we could automatically register them if we had an auto-map policy
    });

    connector.on('registersDiscovered', (discoveryData) => {
      logger.info(`Connector '${sourceId}' discovered registers. Processing for mapping...`);
      this.emit('registersDiscovered', discoveryData);
    });
  }

  async handleIncomingData(sourceId, data) {
    try {
      // ðŸ“Š LOG 1: Dati grezzi ricevuti dalla sorgente
      logger.info(`ðŸ“¥ DATI RICEVUTI da sorgente '${sourceId}':`);
      logger.info(JSON.stringify(data, null, 2));

      // Apply mapping transformations
      const mappedData = await this.mappingEngine.applyMapping(sourceId, data);

      if (mappedData) {
        // ðŸ“Š LOG 2: Dati dopo il mapping
        logger.info(`âœ… DATI MAPPATI per sorgente '${sourceId}':`);
        logger.info(JSON.stringify(mappedData, null, 2));

        // ðŸ“Š LOG 3: Dettagli del mapping applicato
        const mappingDetails = this.mappingEngine.getMappingForSource(sourceId);
        if (mappingDetails) {
          logger.info(`ðŸ”„ MAPPING APPLICATO per '${sourceId}':`);
          logger.info(`   Target: ${mappingDetails.target.type}`);
          logger.info(`   Regole: ${mappingDetails.mappings.length} mapping configurati`);
          mappingDetails.mappings.forEach((mapping, idx) => {
            logger.info(`   [${idx + 1}] ${mapping.sourceField} â†’ ${mapping.targetField} (${mapping.transform || 'direct'})`);
          });
        }

        // Emit mapped data event
        this.emit('data', {
          sourceId,
          originalData: data,
          mappedData,
          timestamp: new Date().toISOString()
        });

        // Store data in configured storages
        if (this.dataStore) {
          await this.dataStore.store({
            sourceId: sourceId,
            timestamp: new Date().toISOString(),
            data: mappedData
          });
        }
      } else {
        logger.debug(`No mapping configured for source '${sourceId}', data not processed`);
      }

    } catch (error) {
      logger.error(`Error handling data from source '${sourceId}':`, error);
      this.emit('dataError', sourceId, error);
    }
  }

  handleConnectorData(sourceId, data) {
    try {
      const connectorInfo = this.connectors.get(sourceId);
      if (!connectorInfo) {
        logger.warn(`Received data from unknown connector: ${sourceId}`);
        return;
      }

      // Update connector stats
      connectorInfo.stats.dataPoints++;
      connectorInfo.lastActivity = new Date();

      // Update global stats
      this.stats.totalDataPoints++;
      this.stats.lastDataReceived = new Date();

      // ðŸ”¥ VERIFICA CHE MAPPING ENGINE SIA INIZIALIZZATO
      if (!this.mappingEngine) {
        logger.error('MappingEngine not initialized');
        return;
      }

      // ðŸ”¥ AUTO-MAPPING: Crea automaticamente il mapping se non esiste
      if (connectorInfo.config.autoMapping && !this.mappingEngine.getMapping(sourceId)) {
        logger.info(`ðŸ”§ Creating automatic mapping for source '${sourceId}'...`);
        this.createAutoMapping(sourceId, data, connectorInfo.config);
      }
      /* // Enrich data with metadata
      const enrichedData = {
        sourceId,
        sourceType: connectorInfo.config.type,
        timestamp: new Date().toISOString(),
        data: data,
        metadata: {
          sourceName: connectorInfo.config.name,
          sourceDescription: connectorInfo.config.description
        }
      }; */

      /* // Map data to Universal Data Model
      try {
        const mappingContext = {
          sourceType: connectorInfo.config.type,
          entityId: `${sourceId}_entity`,
          entityType: connectorInfo.config.entityType
        };

        this.mappingEngine.mapData(data, mappingContext);
        logger.debug(`Data from '${sourceId}' mapped to Universal Data Model`);
      } catch (mappingError) {
        logger.warn(`Failed to map data from '${sourceId}' to Universal Data Model:`, mappingError.message);
        // Continue processing even if mapping fails
      }

      // Send to data processor
      this.dataProcessor.process(enrichedData); */
      // ðŸ”„ CHIAMA IL METODO CON LOGGING
      this.handleIncomingData(sourceId, data);

      /* // Emit raw data event
      this.emit('rawData', enrichedData); */

    } catch (error) {
      logger.error(`Error handling data from connector '${sourceId}':`, error);
      this.stats.totalErrors++;
    }
  }

  determineSubject(data) {
    // 1. Check for AAS specific metadata if mapped
    if (data.metadata && data.metadata.aas && data.metadata.aas.assetId && data.metadata.aas.submodelId) {
      return `aas.update.${data.metadata.aas.assetId}.${data.metadata.aas.submodelId}`;
    }

    // 2. Fallback to generic telemetry subject
    return `ingestor.telemetry.${data.sourceId}`;
  }

  async handleProcessedData(processedData) {
    try {
      let published = false;
      const subject = this.determineSubject(processedData);

      const storagePolicy = process.env.STORAGE_POLICY || 'buffer_on_fail'; // 'always_store' or 'buffer_on_fail'
      const shouldStoreAlways = storagePolicy === 'always_store';

      // 1. Storage: Always Store (if enabled)
      if (shouldStoreAlways) {
        try {
          // We store it without _transportSubject because it's a permanent record, not just a buffer.
          // However, if we want to ensure eventual consistency if NATS fails even in 'always_store' mode, 
          // we might need a separate flag. But usually 'always_store' is for historical records.
          await this.dataStore.store(processedData);
        } catch (dbError) {
          logger.error('Failed to store data in persistent storage:', dbError);
        }
      }

      // 2. Transport: NATS
      if (this.natsTransport && this.natsTransport.isConnected()) {
        try {
          await this.natsTransport.publish(subject, processedData);
          published = true;
          logger.debug(`Published data to NATS: ${subject}`);
        } catch (publishError) {
          logger.warn('Failed to publish to NATS:', publishError.message);
        }
      }

      // 3. Buffer: If failed to publish AND we haven't already stored it effectively for recovery
      // If 'always_store' is on, we rely on the DB having it, but we might need a way to mark it as "unsent" if we want to replay it from DB.
      // For simplicity in this "buffer_on_fail" logic (which seems to be the user's focus for recovery):
      // We will use the DataStore (which might be Redis or Memory) specifically for buffering if NATS failed.
      // If always_store is used (Timescale), we might NOT want to duplicate it into Redis just for buffering, 
      // OR we might want to use Timescale itself as the buffer queue.
      // Given the user request: "recovery procedure that pushes all SENT data... removing all other storage solutions that are not needed"

      // Let's implement the specific request: 
      // Option A: "Always Store" -> TimescaleDB (Historical) + NATS (Realtime)
      // Option B: "Buffer on Fail" -> NATS (Realtime) + Backup Storage (Redis/Timescale) ONLY if NATS fails.

      if (!published) {
        // If we are in 'always_store' mode, the data is already in DB. 
        // But looking it up to resend is expensive/complex without a "published" flag.
        // For now, we will treat the Buffer separately:
        // Buffer is for "Resiliency". Storage is for "History".
        // So we BUFFER if NATS failed.

        await this.dataStore.store({
          ...processedData,
          _transportSubject: subject,
          _bufferedAt: new Date().toISOString(),
          // Add a flag to distinguish from historical records if using same DB?
          // If we use Timescale for BOTH history and buffer, it gets tricky.
          // Assumption: DataStore instance is configured for the BUFFER/Backing store.
          // If 'always_store' is required, maybe a separate "HistoryStore" is needed, or we just utilize the same one but mark regular items.

          // User said: "specify whether to save sensor data or not... alternatively another option that saves ONLY in case of transport failure"
          // This implies the DataStore IS the component we are configuring here.
        });
        logger.info(`Buffered data for ${subject} due to transport unavailability`);
      }

      // Emit processed data event (internal event)
      this.emit('data', processedData);

    } catch (error) {
      logger.error('Error handling processed data:', error);
      this.stats.totalErrors++;
    }
  }

  async flushBuffer() {
    if (!this.natsTransport || !this.natsTransport.isConnected()) return;

    logger.info('Flushing buffered data to NATS...');

    // Check if DataStore supports advanced queries or just basic memory array
    // We need to find items with _transportSubject

    // Basic implementation that works with the current DataStore facade
    // We might need to iterate if it's external storage without explicit query support for this flag

    if (this.dataStore.data && Array.isArray(this.dataStore.data)) {
      // Memory / Simple implementation handling
      const buffer = [...this.dataStore.data];
      let flushedCount = 0;

      // Iterate backwards (oldest first)
      for (let i = buffer.length - 1; i >= 0; i--) {
        const item = buffer[i];
        if (item._transportSubject) {
          try {
            await this.natsTransport.publish(item._transportSubject, item);
            flushedCount++;
            // Remove from buffer (MemoryStore specific)
            // If using external store like Redis/Timescale, we ideally "delete" it

            // In-memory removal:
            const index = this.dataStore.data.indexOf(item);
            if (index > -1) {
              this.dataStore.data.splice(index, 1);
            }

          } catch (e) {
            logger.error('Failed to flush item:', e);
          }
        }
      }
      if (flushedCount > 0) logger.info(`Flushed ${flushedCount} items from memory buffer.`);
      return;
    }

    // External storage handling (Timescale/Redis)
    // We need a way to "get buffered items" and "delete them"
    // Since we don't have explicit "getBuffered" method on adapters, we make a best effort via retrieve/search if possible
    // Or we assume the user accepts the limitation that external storage buffering might require manual replay or advanced impl.
    // BUT the user specifically asked for "recovery procedure".

    // For Redis/Timescale as buffer:
    // We will assume that if we are using them as "Safety Buffer" (not historical),
    // we should fetch all and clear?
    // User: "just recovery... push all un-sent data"

    // If the storage policy is "buffer_on_fail", then EVERYTHING in the store is technically stuff that failed to send (if we only write on fail).
    // So we can iterate and send.

    const storagePolicy = process.env.STORAGE_POLICY || 'buffer_on_fail';

    if (storagePolicy === 'buffer_on_fail') {
      // In this mode, everything in DB is a failed message.
      // We can fetch batches and send.

      try {
        // Get latest 100 (or more)
        const items = await this.dataStore.getLatest(100);
        if (items.length === 0) return;

        logger.info(`Found ${items.length} items in persistent buffer. Attempting flush...`);
        let count = 0;

        // We need to reverse to send oldest first? getLatest returns newest first probably.
        const itemsToSend = items.reverse();

        for (const item of itemsToSend) {
          const subject = item._transportSubject || this.determineSubject(item);
          try {
            await this.natsTransport.publish(subject, item);
            count++;
            // Delete is crucial here to avoid loops.
            // DataStore doesn't have deleteById exposed in the simplified interface we saw?
            // Let's check DataStore methods... it has 'clearBySource' or 'clear'.
            // It does NOT have deleteById. This is a gap.
            // For now, we will stick to 'flush means we successfully sent' and maybe we clear ALL if it's buffer-only mode?
            // Clearing all might be risky if we only fetched 100.
          } catch (e) {
            logger.error('Failed to flush persistent item:', e);
          }
        }

        if (count > 0) {
          logger.info(`Flushed ${count} items. Clearing storage to prevent duplicates (Buffer Mode).`);
          await this.dataStore.clear(); // Nuclear option for now given the API limits
        }

      } catch (err) {
        logger.warn('Error during persistent buffer flush:', err);
      }
    }
  }

  handleConnectorError(sourceId, error) {
    const connectorInfo = this.connectors.get(sourceId);
    if (connectorInfo) {
      connectorInfo.stats.errors++;
    }

    this.stats.totalErrors++;

    logger.error(`Connector '${sourceId}' error:`, error);
    this.emit('connectorError', sourceId, error);
  }

  updateConnectorStatus(sourceId, status) {
    const connectorInfo = this.connectors.get(sourceId);
    if (connectorInfo) {
      const oldStatus = connectorInfo.status;
      connectorInfo.status = status;
      connectorInfo.lastActivity = new Date();

      if (status === 'connected') {
        connectorInfo.stats.connections++;
      }

      if (oldStatus !== status) {
        this.emit('sourceStatusChanged', sourceId, status);
      }
    }
  }

  createAutoMapping(sourceId, sampleData, sourceConfig) {
    try {
      const mappings = [];

      // Analizza i dati ricevuti e crea mapping automatico
      const analyzeData = (data, prefix = '') => {
        for (const key in data) {
          if (data.hasOwnProperty(key)) {
            const value = data[key];
            const fieldPath = prefix ? `${prefix}.${key}` : key;

            if (typeof value === 'object' && value !== null && !Array.isArray(value)) {
              // Ricorsione per oggetti nested
              analyzeData(value, fieldPath);
            } else {
              // Crea mapping per questo campo
              const targetField = fieldPath
                .replace(/[^a-zA-Z0-9]/g, '_')
                .toLowerCase()
                .replace(/^_+|_+$/g, '');

              mappings.push({
                sourceField: fieldPath,
                targetField: targetField,
                transform: this.detectDataType(value)
              });
            }
          }
        }
      };

      analyzeData(sampleData);

      // Crea la configurazione di mapping
      const mappingConfig = {
        sourceId: sourceId,
        target: {
          type: 'ngsi-ld',
          entityType: sourceConfig.name.replace(/[^a-zA-Z0-9]/g, '')
        },
        includeMetadata: true,
        mappings: mappings,
        autoGenerated: true,
        generatedAt: new Date().toISOString()
      };

      // Aggiungi il mapping al MappingEngine
      this.mappingEngine.addMapping(mappingConfig);

      // Salva il mapping su file
      this.saveMappingToFile(mappingConfig);

      logger.info(`âœ… Auto-mapping created for '${sourceId}' with ${mappings.length} fields`);
      logger.info(`ðŸ“„ Mapping saved to config/mapping.json`);

      // Stampa il mapping creato
      console.log('\n' + '='.repeat(80));
      console.log('ðŸ”§ AUTO-MAPPING CREATO:');
      console.log('='.repeat(80));
      console.log(JSON.stringify(mappingConfig, null, 2));
      console.log('='.repeat(80) + '\n');

    } catch (error) {
      logger.error(`Error creating auto-mapping for '${sourceId}':`, error);
    }
  }

  detectDataType(value) {
    if (typeof value === 'boolean') return 'boolean';
    if (typeof value === 'number') {
      return Number.isInteger(value) ? 'number' : 'number';
    }
    if (typeof value === 'string') return 'string';
    if (value instanceof Date) return 'string';
    return 'string';
  }

  async saveMappingToFile(mappingConfig) {
    try {
      const fs = require('fs').promises;
      const path = require('path');
      const configPath = path.join(__dirname, '../../config/mapping.json');

      let existingMappings = [];

      // Leggi i mapping esistenti
      try {
        const content = await fs.readFile(configPath, 'utf8');
        existingMappings = JSON.parse(content);
      } catch (error) {
        // File non esiste, usa array vuoto
        logger.info('Creating new mapping.json file');
      }

      // Rimuovi eventuali mapping esistenti per lo stesso sourceId
      existingMappings = existingMappings.filter(m => m.sourceId !== mappingConfig.sourceId);

      // Aggiungi il nuovo mapping
      existingMappings.push(mappingConfig);

      // Salva su file
      await fs.writeFile(configPath, JSON.stringify(existingMappings, null, 2), 'utf8');

      logger.info(`âœ… Mapping saved to ${configPath}`);

    } catch (error) {
      logger.error('Error saving mapping to file:', error);
    }
  }

  // API Support Methods
  getConnectorStatus() {
    const status = {};

    for (const [connectorId, connectorInfo] of this.connectors.entries()) {
      status[connectorId] = {
        status: connectorInfo.status,
        type: connectorInfo.config.type,
        enabled: connectorInfo.config.enabled,
        lastActivity: connectorInfo.lastActivity,
        stats: {
          dataPoints: connectorInfo.stats.dataPoints,
          errors: connectorInfo.stats.errors,
          connections: connectorInfo.stats.connections
        }
      };
    }

    return status;
  }

  getStatus() {
    return {
      isRunning: this.isRunning,
      natsConnected: this.natsTransport ? this.natsTransport.isConnected() : false,
      stats: {
        totalDataPoints: this.stats.totalDataPoints,
        totalErrors: this.stats.totalErrors,
        startTime: this.stats.startTime,
        lastDataReceived: this.stats.lastDataReceived,
        connectorCount: this.connectors.size,
        uptime: this.stats.startTime ? Date.now() - this.stats.startTime.getTime() : 0
      }
    };
  }


  /**
 * Get latest data from all sources or specific source
 * @param {string} sourceId - Optional source ID filter
 * @param {number} limit - Maximum number of records
 * @returns {Array} Array of data records
 */
  getLatestData(sourceId = null, limit = 100) {
    try {
      if (!this.dataStore) {
        logger.warn('DataStore not available');
        return [];
      }

      // Get all data from store
      const allData = this.dataStore.getAll ? this.dataStore.getAll() : [];

      // Filter by source if specified
      let filteredData = sourceId
        ? allData.filter(d => d.sourceId === sourceId)
        : allData;

      // Sort by timestamp descending
      filteredData.sort((a, b) => {
        const timeA = new Date(a.timestamp || 0).getTime();
        const timeB = new Date(b.timestamp || 0).getTime();
        return timeB - timeA;
      });

      // Apply limit
      return filteredData.slice(0, limit);

    } catch (error) {
      logger.error('Error getting latest data:', error);
      return [];
    }
  }

  /**
   * Get data by source ID
   * @param {string} sourceId - Source ID
   * @param {number} limit - Maximum number of records
   * @returns {Array} Array of data records
   */
  getDataBySource(sourceId, limit = 100) {
    return this.getLatestData(sourceId, limit);
  }

  // Configuration management helpers
  getEnabledSources() {
    return configManager.getEnabledSources();
  }

  getAllSources() {
    return configManager.getSources();
  }

  getStorageInfo() {
    if (this.dataStore && typeof this.dataStore.getStorageInfo === 'function') {
      return this.dataStore.getStorageInfo();
    }

    return {
      type: 'unknown',
      status: 'unknown'
    };
  }

  async start() {
    if (this.isRunning) {
      logger.warn('Data Connector Engine is already running');
      return;
    }

    try {
      logger.info('Starting Data Connector Engine...');

      this.stats.startTime = new Date();
      this.isRunning = true;

      // Connect NATS
      await this.natsTransport.connect();

      // Start all connectors
      const startPromises = Array.from(this.connectors.values()).map(async (connectorInfo) => {
        try {
          await connectorInfo.connector.start();
          logger.info(`Started connector '${connectorInfo.config.id}'`);
          logger.info('ðŸŽ¯ MONITORING ATTIVO - In attesa di dati dal server OPC UA...');
        } catch (error) {
          logger.error(`Failed to start connector '${connectorInfo.config.id}':`, error);
        }
      });

      await Promise.allSettled(startPromises);

      logger.info('Data Connector Engine started successfully');
      this.emit('started');

    } catch (error) {
      logger.error('Failed to start Data Connector Engine:', error);
      this.isRunning = false;
      throw error;
    }
  }

  async stop() {
    if (!this.isRunning) {
      logger.warn('Data Connector Engine is not running');
      return;
    }

    try {
      logger.info('Stopping Data Connector Engine...');

      this.isRunning = false;

      // Stop all connectors
      const stopPromises = Array.from(this.connectors.values()).map(async (connectorInfo) => {
        try {
          await connectorInfo.connector.stop();
          logger.info(`Stopped connector '${connectorInfo.config.id}'`);
        } catch (error) {
          logger.error(`Failed to stop connector '${connectorInfo.config.id}':`, error);
        }
      });

      await Promise.allSettled(stopPromises);

      // Close NATS connection
      if (this.natsTransport) {
        await this.natsTransport.close();
      }

      logger.info('Data Connector Engine stopped successfully');
      this.emit('stopped');

    } catch (error) {
      logger.error('Failed to stop Data Connector Engine:', error);
      throw error;
    }
  }

  async startConnector(sourceId) {
    const connectorInfo = this.connectors.get(sourceId);
    if (!connectorInfo) {
      throw new Error(`Connector '${sourceId}' not found`);
    }

    try {
      await connectorInfo.connector.start();
      logger.info(`Started connector '${sourceId}'`);
      return true;
    } catch (error) {
      logger.error(`Failed to start connector '${sourceId}':`, error);
      throw error;
    }
  }

  async stopConnector(sourceId) {
    const connectorInfo = this.connectors.get(sourceId);
    if (!connectorInfo) {
      throw new Error(`Connector '${sourceId}' not found`);
    }

    try {
      await connectorInfo.connector.stop();
      logger.info(`Stopped connector '${sourceId}'`);
      return true;
    } catch (error) {
      logger.error(`Failed to stop connector '${sourceId}':`, error);
      throw error;
    }
  }

  async restartConnector(sourceId) {
    await this.stopConnector(sourceId);
    await new Promise(resolve => setTimeout(resolve, 1000)); // Wait 1 second
    await this.startConnector(sourceId);
  }

  // Mapping Engine Methods
  getMappingEngine() {
    return this.mappingEngine;
  }

  exportMappedDataToJSON(options = {}) {
    return this.mappingEngine.exportToJSON(options);
  }

  exportMappedDataToNGSILD(options = {}) {
    return this.mappingEngine.exportToNGSILD(options);
  }

  exportMappedDataToTOON(options = {}) {
    return this.mappingEngine.exportToTOON(options);
  }

  getMappedEntity(entityId) {
    return this.mappingEngine.getEntity(entityId);
  }

  getMappedEntitiesByType(type) {
    return this.mappingEngine.getEntitiesByType(type);
  }

  getAllMappedEntities() {
    return this.mappingEngine.getAllEntities();
  }

  getMappingStatistics() {
    return this.mappingEngine.getStatistics();
  }

  clearMappedData() {
    this.mappingEngine.clearAll();
    logger.info('Mapped data cleared');
  }

  async reloadConfiguration() {
    logger.info('Reloading configuration...');

    try {
      // Stop all connectors
      if (this.isRunning) {
        await this.stop();
      }

      // Clear current connectors
      this.connectors.clear();

      // Reload config
      await configManager.reloadConfig();

      // Reinitialize connectors
      await this.initializeConnectors();

      // Restart if it was running
      if (this.isRunning) {
        await this.start();
      }

      logger.info('Configuration reloaded successfully');
      this.emit('configurationReloaded');

    } catch (error) {
      logger.error('Failed to reload configuration:', error);
      throw error;
    }
  }
}



module.exports = DataConnectorEngine;
